{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b8db48",
   "metadata": {},
   "source": [
    "# Model customisation & experiments\n",
    "\n",
    "This is an alternative to running `./main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd25c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the server isn't running already, start it\n",
    "#!make -C ../ server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95421b",
   "metadata": {},
   "source": [
    "## Custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5463954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TICK = 100 # speed of the game [milliseconds]\n",
    "TETRIS_SERVER_URL = \"http://localhost:8888\"\n",
    "\n",
    "# Silence the cretinous nagging of TensorFlow:\n",
    "# \"This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
    "# \"To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # Note: needs to be set before importing tensorflow\n",
    "\n",
    "# in Jupyter notebooks, we need the async version of playwright\n",
    "from tetris_control_async import control as ctrl\n",
    "from model import Model\n",
    "\n",
    "\n",
    "num_iterations = 1 # number of games to play per training session\n",
    "off_policy = True # set to false to train on-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0171a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "my_model = tf.keras.Sequential()\n",
    "\n",
    "# Comment this out so that the shape is not specified until the model is first instantiated\n",
    "# myShape = encode_state(get_state()).shape\n",
    "# model.add(tf.keras.layers.Input(input_shape=myShape))\n",
    "\n",
    "my_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "numOutputs = 4 * 10 # 4 rotations Ã— 10 positions\n",
    "my_model.add(tf.keras.layers.Dense(numOutputs, activation='softmax'))\n",
    "\n",
    "# Compile the model.\n",
    "my_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713da5f5",
   "metadata": {},
   "source": [
    "## Custom model trainer\n",
    "\n",
    "Note: Use `self.maybeLoadWeights()` and `self.maybeSaveWeights()` to persist the model state across invocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7eb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this to shut the verbose output of model.fit() & model.predict() right up\n",
    "import sys\n",
    "class stdout_redirected(object):\n",
    "    def __init__(self, to=\"/dev/null\"):\n",
    "        self.to = to\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.sys_stdout = sys.stdout\n",
    "        sys.stdout = open(self.to, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self.sys_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09411ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from move import Move\n",
    "from reward import Reward\n",
    "\n",
    "def my_trainer(self, num_iterations, offPolicy=True):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Track the elapsed time.\n",
    "    self.start_time = time.time()\n",
    "\n",
    "    games_played = 0\n",
    "    print (\"Iteration: \" + str(games_played + 1) + \"/\" + str(num_iterations))\n",
    "    while games_played < num_iterations:\n",
    "        if self.control.is_game_over():\n",
    "            self.control.new_game()\n",
    "            games_played += 1\n",
    "            if games_played < num_iterations:\n",
    "                print (\"Iteration: \" + str(games_played + 1) + \"/\" + str(num_iterations))\n",
    "            else:\n",
    "                print (\"\\nTraining complete.\")\n",
    "            continue\n",
    "\n",
    "        # Get the current state of the game.\n",
    "        state = self.state.get_state()\n",
    "        state_encoded = self.state.encode_state(state)\n",
    "\n",
    "        # Get all the possible plays.\n",
    "        move = Move(self.control)\n",
    "        possible_plays = move.all_possible_end_states()\n",
    "        boards_after = [play[\"board_after\"] for play in possible_plays]\n",
    "        rewards = [Reward(state, board).get_reward() for board in boards_after]\n",
    "        batch_size = len(rewards) # 40\n",
    "        rewards_softmax = self.softmax(np.array(rewards).reshape(1, batch_size))\n",
    "\n",
    "        # Choose an action.\n",
    "        with stdout_redirected(\"/dev/null\"):\n",
    "            prediction = self.model.predict(state_encoded)\n",
    "        self.maybeLoadWeights() # We have called the model, so now the model knows its input shape, so we can load weights\n",
    "        if offPolicy:\n",
    "            actionChoice = np.argmax(rewards)\n",
    "            assert(actionChoice == np.argmax(rewards_softmax))\n",
    "        else:\n",
    "            actionChoice = np.argmax(prediction)\n",
    "\n",
    "        #print(\"piece:\", state[\"piece\"][\"type\"], \"position:\", possible_plays[actionChoice][\"position\"], \"rotation:\", possible_plays[actionChoice][\"rotation\"], \"reward:\", rewards[actionChoice], \"(\" + str(rewards[np.argmax(prediction)] - rewards[np.argmax(rewards)]) + \")\")\n",
    "        print(state[\"piece\"][\"type\"], end=\"\")\n",
    "\n",
    "        # Take the action.\n",
    "        motion = possible_plays[actionChoice][\"motion\"]\n",
    "        move.perform_motion(motion, True)\n",
    "\n",
    "        time.sleep(self.control.get_tick()/1000.0) # we don't collect the state after the tick, so this is just for show -- XXX there seems to be some race condition and if we don't sleep(), the training moves are weird\n",
    "\n",
    "        # Update the model.\n",
    "        with stdout_redirected(\"/dev/null\"):\n",
    "            self.model.fit(state_encoded, rewards_softmax, epochs=1, batch_size=1, verbose=0)\n",
    "        self.maybeSaveWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf4c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.playwright: <playwright._impl._playwright.Playwright object at 0x7f8bc400f5b0>\n",
      "Iteration: 1/1\n",
      "Loaded weights from file autopilot-model-weights.h5\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               58112     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 40)                10280     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,392\n",
      "Trainable params: 68,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model summary: None (None, 226) (None, 40)\n",
      "OTSSJOIIZLLOTOIJSSTZTOJTISLIOJJLJZOZTOILTIZSZSOLJTJSSTOJOJTTISZJIJZLOILTTLSSTLTTILSSTSOILLSSITLJJJISISSTJZIZJZLLZZSJJSJIOJIJJLZZOLLZJZTITraining complete.\n"
     ]
    }
   ],
   "source": [
    "async with ctrl(TETRIS_SERVER_URL) as control:\n",
    "    Model.train_model = my_trainer # override Model.train_model()\n",
    "    model = Model(control)\n",
    "    control.set_tick(TICK)\n",
    "    model.model = my_model # override Model.model\n",
    "    model.train_model(num_iterations, off_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183633b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
